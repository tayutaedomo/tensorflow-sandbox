{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference\n",
    "  - Blog: https://work-in-progress.hatenablog.com/entry/2019/04/06/113629\n",
    "  - Source: https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "  - Source: https://github.com/eriklindernoren/Keras-GAN/pull/117\n",
    "    - Added functionality to save/load Keras model for intermittent training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/images', exist_ok=True)\n",
    "os.makedirs('data/saved_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.history = pd.DataFrame({}, columns=['d_loss', 'acc', 'g_loss'])\n",
    "\n",
    "        self.img_save_dir = 'data/images'\n",
    "        self.model_save_dir = 'data/saved_models'\n",
    "        self.discriminator_name = 'discriminator_model'\n",
    "        self.generator_name = 'generator_model'\n",
    "        self.combined_name = 'combined_model'\n",
    "\n",
    "        self.discriminator = None\n",
    "        self.generator = None\n",
    "        self.combined = None\n",
    "\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "    def init(self, loading=False):\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "        if loading:\n",
    "            self.load_model_weight(self.discriminator_name)\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "        if loading:\n",
    "            self.load_model_weight(self.generator_name)\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.summary()\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        if loading:\n",
    "            self.load_model_weight(self.combined_name)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=-1, save_interval=-1):\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "        #print(X_train.shape)\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        #print(X_train.shape)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        print(datetime.datetime.now().isoformat(), 'Epoch Start')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            self.history = self.history.append({'d_loss': d_loss[0], 'acc': d_loss[1], 'g_loss': g_loss}, ignore_index=True)\n",
    "\n",
    "            if sample_interval > 0 and epoch % sample_interval == 0:\n",
    "                print(datetime.datetime.now().isoformat(), '%d [D loss: %f, acc.: %.2f%%] [G loss: %f]' % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "            if save_interval > 0 and epoch != 0 and epoch % save_interval == 0:\n",
    "                self.save_model_weights_all()\n",
    "                \n",
    "        print(datetime.datetime.now().isoformat(), 'Epoch End')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "\n",
    "        file_name = '{}.png'.format(epoch)\n",
    "        file_path = os.path.join(self.img_save_dir, file_name)\n",
    "        fig.savefig(file_path)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_hisotry(self, columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            columns = ['d_loss', 'acc', 'g_loss']\n",
    "        self.history[columns].plot()\n",
    " \n",
    "    def save_model_weights_all(self):\n",
    "        self.save_model_weights(self.discriminator, self.discriminator_name)\n",
    "        self.save_model_weights(self.generator, self.generator_name)\n",
    "        self.save_model_weights(self.combined, self.combined_name)\n",
    "\n",
    "    def save_model_weights(self, model, model_name):\n",
    "        weights_path = os.path.join(self.model_save_dir, '{}.h5'.format(model_name))\n",
    "        model.save_weights(weights_path)\n",
    "\n",
    "        print('Weights saved.', model_name)\n",
    "\n",
    "    def load_model_weight(self, model_name):\n",
    "        model = None\n",
    "\n",
    "        if model_name == self.discriminator_name:\n",
    "            model = self.discriminator\n",
    "        elif model_name == self.generator_name:\n",
    "            model = self.generator\n",
    "        elif model_name == self.combined_name:\n",
    "            model = self.combined\n",
    "\n",
    "        if not model:\n",
    "            print('Model is not initialized.', model_name)\n",
    "            return\n",
    "\n",
    "        weights_path = os.path.join(self.model_save_dir, '{}.h5'.format(model_name))\n",
    "\n",
    "        if not os.path.exists(weights_path):\n",
    "            print('Not found h5 file.', model_name)\n",
    "            return\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "        print('Weights loaded.', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Weights loaded. discriminator_model\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Weights loaded. generator_model\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model_19 (Model)             (None, 28, 28, 1)         1493520   \n",
      "_________________________________________________________________\n",
      "model_18 (Model)             (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 2,027,025\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 537,089\n",
      "_________________________________________________________________\n",
      "Weights loaded. combined_model\n"
     ]
    }
   ],
   "source": [
    "#gan.init()\n",
    "gan.init(loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-20T16:40:40.930278 Epoch Start\n",
      "2020-07-20T16:40:41.984151 0 [D loss: 0.017323, acc.: 100.00%] [G loss: 4.603593]\n",
      "2020-07-20T16:40:44.686288 50 [D loss: 0.043630, acc.: 100.00%] [G loss: 4.487054]\n",
      "2020-07-20T16:40:47.771228 100 [D loss: 0.132579, acc.: 96.88%] [G loss: 3.762897]\n",
      "2020-07-20T16:40:50.378977 150 [D loss: 0.180828, acc.: 95.31%] [G loss: 2.896070]\n",
      "2020-07-20T16:40:52.967509 200 [D loss: 0.512427, acc.: 68.75%] [G loss: 2.078963]\n",
      "2020-07-20T16:40:55.735869 250 [D loss: 0.722651, acc.: 42.19%] [G loss: 0.785459]\n",
      "2020-07-20T16:40:58.321568 300 [D loss: 0.701244, acc.: 42.19%] [G loss: 0.665993]\n",
      "2020-07-20T16:41:00.907402 350 [D loss: 0.661094, acc.: 50.00%] [G loss: 0.774907]\n",
      "2020-07-20T16:41:03.797166 400 [D loss: 0.669651, acc.: 50.00%] [G loss: 0.668935]\n",
      "2020-07-20T16:41:06.421565 450 [D loss: 0.671747, acc.: 43.75%] [G loss: 0.665527]\n",
      "2020-07-20T16:41:08.977252 Epoch End\n",
      "Weights saved. discriminator_model\n",
      "Weights saved. generator_model\n",
      "Weights saved. combined_model\n"
     ]
    }
   ],
   "source": [
    "#gan.train(epochs=500, batch_size=32, sample_interval=50, save_interval=50)\n",
    "gan.train(epochs=500, batch_size=32, sample_interval=50)\n",
    "gan.save_model_weights_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-20T16:41:43.543362 Epoch Start\n",
      "2020-07-20T16:41:43.602838 0 [D loss: 0.646707, acc.: 60.94%] [G loss: 0.719015]\n",
      "2020-07-20T16:41:46.254114 50 [D loss: 0.644686, acc.: 56.25%] [G loss: 0.700990]\n",
      "2020-07-20T16:41:49.387355 100 [D loss: 0.638763, acc.: 60.94%] [G loss: 0.792076]\n",
      "2020-07-20T16:41:52.140002 150 [D loss: 0.607266, acc.: 65.62%] [G loss: 0.807922]\n",
      "2020-07-20T16:41:54.762389 200 [D loss: 0.630868, acc.: 59.38%] [G loss: 0.807604]\n",
      "2020-07-20T16:41:57.533070 250 [D loss: 0.611223, acc.: 65.62%] [G loss: 0.824006]\n",
      "2020-07-20T16:42:00.164798 300 [D loss: 0.626919, acc.: 67.19%] [G loss: 0.855750]\n",
      "2020-07-20T16:42:02.789074 350 [D loss: 0.623483, acc.: 62.50%] [G loss: 0.778964]\n",
      "2020-07-20T16:42:05.617582 400 [D loss: 0.619047, acc.: 68.75%] [G loss: 0.871488]\n",
      "2020-07-20T16:42:08.233352 450 [D loss: 0.619326, acc.: 76.56%] [G loss: 0.822424]\n",
      "2020-07-20T16:42:10.908828 500 [D loss: 0.624528, acc.: 70.31%] [G loss: 0.896437]\n",
      "2020-07-20T16:42:13.529280 550 [D loss: 0.609691, acc.: 67.19%] [G loss: 0.859317]\n",
      "2020-07-20T16:42:16.470823 600 [D loss: 0.597014, acc.: 70.31%] [G loss: 0.881013]\n",
      "2020-07-20T16:42:19.188759 650 [D loss: 0.583511, acc.: 76.56%] [G loss: 0.896109]\n",
      "2020-07-20T16:42:21.796963 700 [D loss: 0.579728, acc.: 71.88%] [G loss: 0.961091]\n",
      "2020-07-20T16:42:24.397138 750 [D loss: 0.557666, acc.: 76.56%] [G loss: 1.003329]\n",
      "2020-07-20T16:42:27.359921 800 [D loss: 0.566582, acc.: 75.00%] [G loss: 0.982712]\n",
      "2020-07-20T16:42:29.971031 850 [D loss: 0.566183, acc.: 73.44%] [G loss: 0.955344]\n",
      "2020-07-20T16:42:32.581164 900 [D loss: 0.558283, acc.: 75.00%] [G loss: 1.013446]\n",
      "2020-07-20T16:42:35.184796 950 [D loss: 0.630461, acc.: 68.75%] [G loss: 0.916386]\n",
      "2020-07-20T16:42:37.769975 Epoch End\n",
      "Weights saved. discriminator_model\n",
      "Weights saved. generator_model\n",
      "Weights saved. combined_model\n"
     ]
    }
   ],
   "source": [
    "gan.train(epochs=1000, batch_size=32, sample_interval=50)\n",
    "gan.save_model_weights_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
